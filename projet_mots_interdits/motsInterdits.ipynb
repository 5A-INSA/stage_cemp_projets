{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c201656",
   "metadata": {},
   "source": [
    "# Projet Mots interdits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd6711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies nécessaires\n",
    "# --------------------------------------\n",
    "# ---- Calculs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ---- Traitement de texte\n",
    "import re #regex cheat sheet : http://www.rexegg.com/regex-quickstart.html\n",
    "import unicodedata\n",
    "# ---- NLP\n",
    "import spacy #pip install spacy\n",
    "import nltk #pip install nltk\n",
    "# ---- Sauvegarde & Excel \n",
    "import pickle\n",
    "import xlsxwriter #pip install xlsxwriter\n",
    "import time #mesure du temps d'exécution\n",
    "\n",
    "#conda install openpyxl #pour lire du xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaffec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales \n",
    "# --------------------------------------\n",
    "PATH = \"C:/Users/A3193307/Groupe BPCE/CEMP - Data & Décisionnel - Data Science/Analyse Mots Interdits\"\n",
    "UTILS = PATH + \"/utilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1c265",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632b2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des fichiers\n",
    "# --------------------------------------\n",
    "# Lecture de la base Osirisk dans laquelle il faudra détecter les mots interdits (un peu long à charger)\n",
    "# df_osirisk = pd.read_excel(\"data/Osirisk_export_20230524 - mots interdits RGPD.xlsx\")\n",
    "\n",
    "# Sauvegarde de la base Osirisk sous un format plus rapide à charger\n",
    "# df_osirisk.to_pickle(\"temp_result/df_osirisk.pkl\")\n",
    "df_osirisk = pd.read_pickle(\"temp_result/df_osirisk.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8f967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de la 1ère table de mots interdits (contrôle à postériori)\n",
    "df_posteriori = pd.read_excel(\"data/2023 01 20 Dictionnaire contrôle a posteriori.xlsx\")\n",
    "\n",
    "# Lecture de la 2eme table de mots interdits (contrôle à priori)\n",
    "df_priori = pd.read_excel(\"data/2023 01 20 Dictionnaire contrôle a priori.xlsx\")\n",
    "\n",
    "# Lecture de la liste des pénoms de 1900-2021\n",
    "# source : https://www.insee.fr/fr/statistiques/2540004\n",
    "df_prenoms = pd.read_csv(\"data/prenoms_France_1900-2021.csv\",encoding = 'ANSI',delimiter = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925409fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement des fichiers récupérés\n",
    "# --------------------------------------\n",
    "# ------ 1) Mots interdits\n",
    "df_priori = df_priori.loc[2:]\n",
    "# liste de tous les mots interdits (contrôle à priori + contrôle à postériori)\n",
    "motsInterdits = list(df_priori[\"CODEMS\"].values) + list(df_posteriori[\"Mots interdits\"].values)\n",
    "# on retire tous les mots en double\n",
    "motsInterdits = list(set(motsInterdits))\n",
    "\n",
    "# ------ 2) Table Osirik\n",
    "# liste des colonnes pour lequelles regarder les mots interdits\n",
    "columns = [\"Libellé Incident\", \"Description\",  \"Réclamation\", \"Local\", \"Client\", \"Lieu\"]\n",
    "# réduction de la table aux colonnes intéressantes\n",
    "df_osirisk = df_osirisk[[\"# Incident\"] + columns]\n",
    "\n",
    "# ------ 3) Table des prénoms\n",
    "# on retire les valeurs nulles\n",
    "df_prenoms = df_prenoms.dropna()\n",
    "# on récupère la liste des prénoms\n",
    "prenoms = list(df_prenoms[\"preusuel\"].unique())\n",
    "# on retire le 1er élément qui n'est pas un prénom\n",
    "prenoms = prenoms[1:]\n",
    "# on retire les prénoms composés d'une seule lettre ou de deux lettres (ex: 'A', 'De', 'El',...)\n",
    "prenoms = [p for p in prenoms if len(p)>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1340f85",
   "metadata": {},
   "source": [
    "### Fonction pour le pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc989d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt,treatment=[]):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée un string et retire : \n",
    "    - la poncutation si treatment=['p']\n",
    "    - les majuscules si treatment=['m']\n",
    "    - les accents si treatment=['a']\n",
    "    - les espaces superflus si treatment=['s']\n",
    "    - les chiffres si treatment=['d']\n",
    "    - les valeurs nan si treatment=['n']\n",
    "    - la ponctuation + les majuscules is treatment=['p','m']\n",
    "    - ...\n",
    "    - tout ce qui est mentionné ci-dessus si treatment=[]\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    - txt (string) : texte à modifier\n",
    "    - treatment (list of caracters) : liste indiquant quel traitement\n",
    "      doit être effectué sur le texte. \n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "     - txt (string) : modifié\n",
    "    \"\"\"\n",
    "    # si treatment=[], on effectue tous les traitements de texte\n",
    "    if len(treatment)==0:\n",
    "        treatment = ['p','m','a','s','d','n']\n",
    "    \n",
    "    # Traitements de texte : \n",
    "    # ------------------------------\n",
    "    for treat in treatment:\n",
    "        \n",
    "        # Transforme txt en string\n",
    "        txt = str(txt)\n",
    "        \n",
    "        # Retire la ponctuation\n",
    "        if treat == 'p':\n",
    "            txt = re.sub(r'[^\\w\\s]', '',txt)\n",
    "        \n",
    "        # Met tout en minuscule \n",
    "        if treat == 'm':\n",
    "            txt = txt.lower()\n",
    "        \n",
    "        # Retire les chiffres\n",
    "        if treat == 'd':\n",
    "            txt = re.sub('\\d+', '', txt)\n",
    "       \n",
    "        # Retire les espaces superflus\n",
    "        if treat == 's':\n",
    "            # Remplace de multiples espaces possibles à l'intérieur du string par un seul espace\n",
    "            txt = re.sub(\"\\s\\s+\" , \" \", txt)\n",
    "            # Retire les espaces au début et à la fin du string\n",
    "            txt = txt.strip()\n",
    "        \n",
    "        # Retire les accents\n",
    "        if treat == 'a':\n",
    "            txt = ''.join(c for c in unicodedata.normalize('NFD', txt) if unicodedata.category(c) != 'Mn')\n",
    "        \n",
    "        # Si le texte est nan, on met un string vide à la place\n",
    "        if treat == 'n':\n",
    "            if txt == 'nan' : txt = ''\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b44c38",
   "metadata": {},
   "source": [
    "### Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe47839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement du texte pour chaque colonne de la table\n",
    "# -----------------------------\n",
    "df_osirisk_1 = df_osirisk.copy()\n",
    "for col in columns: \n",
    "    df_osirisk_1[col] = df_osirisk_1[col].apply(lambda x : preprocess_text(x))\n",
    "\n",
    "# Pré-traitement des mots interdits\n",
    "# -----------------------------    \n",
    "motsInterdits_1 = motsInterdits.copy()\n",
    "motsInterdits_1 = [preprocess_text(mot) for mot in motsInterdits_1]\n",
    "\n",
    "# Pré-traitement des prénoms\n",
    "# -----------------------------    \n",
    "prenoms_1= prenoms.copy()\n",
    "prenoms_1 = [preprocess_text(prenom) for prenom in prenoms_1]\n",
    "# comme on a retiré les accents et les majuscules, cela a pu créer \n",
    "# doublons dans la liste des prénoms. On supprime ces doublons\n",
    "prenoms_1 = list(dict.fromkeys(prenoms_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b4f44",
   "metadata": {},
   "source": [
    "## Méthode 1 : sans lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324fe2d",
   "metadata": {},
   "source": [
    "**Méthode utilisée dans cette partie :**\n",
    "\n",
    "Dans cette partie, nous recherchons les mots de la liste motsInterdits présents dans la base Osirisk. Pour ce faire, nous effectuons d'abord un pré-traitement du texte de la base Osirisk et des motsInterdits (retrait accents, ponctuation, chiffres, valeurs manquantes, majuscules) puis nous recherchons les mots de motsInterdits (après pré-traitement) qui sont dans Osirisk (après pré-traitement). Cependant, il arrive que certains prénoms comme 'Nègre Véronique' soient considérés à tort comme des mots interdits. De fait, nous téléchargons une liste de tous les prénoms de 1900-2021. Lorsque nous recherchons les motsInterdits qui sont dans Osirisk, nous considérons qu'un mot n'est pas un mot interdit si celui-ci est précédé ou suivi d'un mot qui se trouve dans la liste des prénoms. \\\n",
    "Rq: Nous aurions pu utiliser les NER (reconnaissance d'entité nommée) pour déterminer si un mot est un prénom ou pas, mais de nombreux prénoms ne sont pas reconnus avec le NER (ex du prénom Urbain). Au prélable, nous avons également retiré les accents et les majuscules de la liste des prénoms et nous avons supprimé les prénoms composé de moins de 2 lettres (ex : 'A', 'El'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6beb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec2min(time):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée un argument time en secondes\n",
    "    et renvoie un tuple contenant la valeur de time en (minutes,secondes).\n",
    "    \"\"\"\n",
    "    minutes = int(time//60) \n",
    "    secondes = round(time - minutes*60,3)\n",
    "    return (minutes,secondes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b84eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ban_word_if_name(txt,mot_interdit,prenoms,debug=False):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée un string \"txt\" et \n",
    "    permet de dire si dans \"txt\", le mot qui précède le mot \"mot_interdit\"\n",
    "    ou le mot qui suit le mot \"mot_interdit\" fait partie de la liste \"prenoms\".    \n",
    "    \n",
    "    Intput:\n",
    "    ------\n",
    "    - txt (string) : string dans lequel rechercher la présence de prénoms.\n",
    "      txt contient le mot mot_interdit.\n",
    "    - mot_interdit (sring) : mot interdit contenu dans txt.\n",
    "    - prenoms (list of strings) : liste de prénoms.\n",
    "    - debug (bool) : si True, on affiche des infos.\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    - found_name (bool) : si True, le mot_interdit est entouré d'un prénom\n",
    "     (collé avant ou apèrs mot_interdit).\n",
    "    - name (string) : prenom trouvé dans le cas où found_name = True.\n",
    "    \n",
    "    ================================= Notes =================================\n",
    "    Pour reconnaître si c'est un prénom, on aurait pu utiliser la \n",
    "    Reconnaissance d’entités nommées (NER) de Spacy avec le code suivant : \n",
    "    doc = nlp(sentence)\n",
    "    return [(X.text, X.label_) for X in doc.ents]\n",
    "    Cependant, cela ne fonctionne pas pour un très grand nombre de prénoms. \n",
    "    =========================================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----- initialisation\n",
    "    name = ''    \n",
    "    found_name = False   \n",
    "    \n",
    "    # ----- on extrait de txt, le mot avant et après le mot_interdit \n",
    "    before_keyword, keyword, after_keyword = txt.partition(mot_interdit) #mot_interdit=keyword\n",
    "\n",
    "    # vérification que le mot avant keyword existe bien et n'est pas un espace\n",
    "    if len(before_keyword) > 0 and before_keyword.isspace() == False:\n",
    "        before_keyword = before_keyword.split()[-1] #on ne veut que le mot avant keyword et pas toute l'expression\n",
    "    # vérification que le mot avant keyword existe bien et n'est pas un espace\n",
    "    if len(after_keyword) > 0 and after_keyword.isspace() == False:\n",
    "        after_keyword = after_keyword.split()[0] #on ne veut que le mot après keyword et pas toute l'expression    \n",
    "\n",
    "    # ----- affichage si debug = True \n",
    "    if debug :\n",
    "        print(\"\\nmot interdit :\", keyword)\n",
    "        print(\"mot trouvé avant le mot interdit : \",before_keyword)\n",
    "        print(\"mot trouvé après le mot interdit :\",after_keyword)\n",
    "    \n",
    "    # ----- parcourt des éléments de la liste prenoms et verification\n",
    "    #       si before_keyword ou after_keyword est un prénom\n",
    "    j=0\n",
    "    while not(found_name) and j < len(prenoms):\n",
    "        p = prenoms[j]\n",
    "        if p==before_keyword or p==after_keyword:\n",
    "            found_name = True\n",
    "            name = p \n",
    "        else :\n",
    "            j+=1\n",
    "    \n",
    "    return found_name, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "529a51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trouve_mot_interdit_1(txt_list,motsInterdits,prenoms):\n",
    "    \"\"\"\n",
    "    Trouve les mots interdits.\n",
    "    Cette fonction trouve dans la liste de phrases \"txt_list\", les mots interdits\n",
    "    de la liste \"motsInterdits\" qui ne sont pas dans la liste \"prenoms\".\n",
    "    On effectue une simple comparaison pour rechercher les mots interdits :\n",
    "    les mots de \"motsInterdits\" qui sont exactement dans \"txt_list\" sont renvoyés,\n",
    "    sauf si un mot de \"motsInterdits\" est suivi ou précédé par un mot de \"prenoms\".\n",
    "    >> Utilise la fonction ban_word_if_name()\n",
    "     \n",
    "    Intput:\n",
    "    -------\n",
    "    - txt_list (list of string) : liste de phrases pour lesquelles on doit \n",
    "      détecter les mots interdits.\n",
    "    - motsInterdits (list of string) : liste contenant les mots interdits.\n",
    "    - prenoms (list of string) : liste de prénoms.  \n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    - dict_interdit (dict) : dictionnaire avec en key, l'indice du mot interdit dans la liste \n",
    "      \"txt_list\" et en value, le mot interdit. Lorsqu'il y a plusieurs mots interdits pour un \n",
    "      même indice, on a en value une liste de mots interdits. \n",
    "      \n",
    "    Référence:\n",
    "    ---------\n",
    "    Adding a value to a specific key in a dictionary in Python :\n",
    "    - https://stackoverflow.com/questions/67799632/adding-a-value-to-a-specific-key-in-a-dictionary-in-python\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionnaire avec en key, l'indice du mot interdit dans la liste \n",
    "    # \"txt_list\" et en value, le mot interdit. \n",
    "    # Comme il peut y avoir plusieurs mots interdits pour un même indice,\n",
    "    # certains éléments du dictionnaire sont des listes\n",
    "    dict_interdit = {}\n",
    "    \n",
    "    i = 0 \n",
    "    \n",
    "    # pacours de chaque élément de txt_list\n",
    "    for txt in txt_list:\n",
    "        \n",
    "        \n",
    "        # parcours de chaque mot interdit\n",
    "        for word in motsInterdits: \n",
    "            # on regarde si le mot interdit est dans le texte\n",
    "            found_compteur = 0 #compteur pour dire cb de mots du mots interdits sont le dans texte\n",
    "            #(le mot interdit peut être composé de plusieurs mots)\n",
    "            for w in word.split(): \n",
    "                if w in txt.split():\n",
    "                    found_compteur+=1\n",
    "            if len(word.split()) == found_compteur: #si tous les mots du mot interdit sont dans le texte\n",
    "\n",
    "                # vérification que le mot interdit n'est pas en réalité un prénom\n",
    "                found_name, _ = ban_word_if_name(txt,word,prenoms)\n",
    "                \n",
    "                if not(found_name): # si le mot interdit n'est pas un prénom\n",
    "                    \n",
    "                    # sauvegarde du mot interdit et de l'indice de ce mot dans la liste \"txt_list\" :\n",
    "                    # ----------------------------\n",
    "                    # dans dict_interdit, key=i et value = word\n",
    "                    # si la key n'est pas déjà dans le dict, on l'ajoute\n",
    "                    if not(i in dict_interdit):\n",
    "                        dict_interdit[i] = word\n",
    "                    # si la key est déjà dans le dict ie si dict[key] existe...    \n",
    "                    else :\n",
    "                        #...si dict[key] est déjà une liste, on ajoute la value à cette liste\n",
    "                        if type(dict_interdit[i]) is list:\n",
    "                            dict_interdit[i].append(word)\n",
    "                        #...si dict[key] est juste une valeur (=pas de liste),\n",
    "                        # on crée une liste avec la valeur déjà présente + la nouvelle value à ajouter\n",
    "                        else: \n",
    "                            tmp = []\n",
    "                            tmp.append(dict_interdit[i])\n",
    "                            tmp.append(word)\n",
    "                            dict_interdit[i] = tmp\n",
    "                    # ----------------------------                   \n",
    "        \n",
    "        i+=1\n",
    "               \n",
    "\n",
    "    return dict_interdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04816c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_interdit(table_osirisk,columns,motsInterdits,prenoms):\n",
    "    \"\"\"\n",
    "    Cette fonction crée une liste où chaque élément est une dataframe contenant les mots \n",
    "    interdits d'une colonne données de la base Osirisk. \n",
    "    (parmi les colonnes \"Libellé Incident\", \"Description\", \"Réclamation\", \"Local\", \"Client\", \"Lieu\"). \n",
    "    Chaque élément de la liste est une dataframe contenant les colonnes suivantes :\n",
    "    - numéro client\n",
    "    - mots interdits : mots interdits trouvés \n",
    "    - texte : texte dans lequel on a trouvé les mots interdits\n",
    "    - index osirik : indice du mot interdit dans la dataframe df_osirisk. \n",
    "    \n",
    "    >>> Utilise la fonction trouve_mot_interdit_1() et sec2min().\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    - table_osirisk (dataframe) : dataframe contenant la table osirik.\n",
    "    - columns (list of string) : liste des colonnes de table_osirisk pour \n",
    "      lesquelles rechercher les mots interdits. \n",
    "    - motsInterdits (list of string) : liste contenant les mots interdits \n",
    "      à trouver dans la table Osirisk.\n",
    "    - prenoms (list of string) : liste des prénoms tels que un mot interdit\n",
    "      qui précède ou qui suit un prénom n'est pas considéré comme un mot interdit.  \n",
    "      \n",
    "    Output:\n",
    "    -------\n",
    "    - df_interdit : liste de dataframes où chaque élément représente la dataframe\n",
    "    des mots interdits de la base Osirik pour une colonne donnée \n",
    "    (colonnes: \"Libellé Incident\", \"Description\",  \"Réclamation\", \"Local\", \"Client\", \"Lieu\")\n",
    "    Chaque dataframe de la liste contien les colonnes suivantes :\n",
    "    - numéro client\n",
    "    - mots interdits : mots interdits trouvés \n",
    "    - texte : texte dans lequel on a trouvé les mots interdits\n",
    "    - index osirik : indice du mot interdit dans la dataframe df_osirisk. \n",
    "    \"\"\"\n",
    "    # ====================\n",
    "    # Temps de départ\n",
    "    st = time.time()\n",
    "    print(\"Debut Analyse mots interdits...\")\n",
    "    # ====================\n",
    "    \n",
    "    # liste de dataframes où chaque élément représente la dataframe\n",
    "    # des mots interdits de la base Osirik pour une colonne donnée \n",
    "    # (colonnes: \"Libellé Incident\", \"Description\",  \"Réclamation\", \"Local\", \"Client\", \"Lieu\")\n",
    "    df_interdit = []\n",
    "\n",
    "    for col in columns:    \n",
    "        # calcul des mots interdits pour chaque colonne\n",
    "        if print_progress:\n",
    "            print(\"\\ncolonne\",col)\n",
    "        \n",
    "        dict_interdit = trouve_mot_interdit_1(txt_list = table_osirisk[col],\n",
    "                                              motsInterdits = motsInterdits,\n",
    "                                              prenoms = prenoms,\n",
    "                                              print_progress=print_progress)\n",
    "\n",
    "        # création d'une dataframe vide pour chaque colonne\n",
    "        df_i = pd.DataFrame()\n",
    "\n",
    "        # remplissage de la dataframe pour chaque colonne\n",
    "        for j in range(len(dict_interdit)):\n",
    "            idx =  list(dict_interdit.keys())[j]\n",
    "            row_df= {'numero incident': table_osirisk[\"# Incident\"].iloc[idx],\n",
    "                     'mots interdits': dict_interdit[idx],\n",
    "                     'texte': table_osirisk[col].iloc[idx],\n",
    "                     'index osirik' : idx}\n",
    "\n",
    "            df_i = pd.concat([df_i, pd.DataFrame([row_df])], ignore_index=True)\n",
    "\n",
    "        df_interdit.append(df_i)\n",
    "    \n",
    "    # ====================\n",
    "    print(\"\\n\\n... fin !\")\n",
    "    # Temps de fin\n",
    "    et = time.time()\n",
    "    # Temps d'exécution\n",
    "    execution_time = et - st\n",
    "    minutes,secondes = sec2min(execution_time)\n",
    "    print(\"Temps d'exécution : {} minutes {} secondes\".format(minutes,secondes))\n",
    "    # ====================\n",
    "    \n",
    "    return df_interdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "57213971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debut Analyse mots interdits...\n",
      "\n",
      "colonne Libellé Incident\n",
      "\n",
      "colonne Description\n",
      "\n",
      "colonne Réclamation\n",
      "\n",
      "colonne Local\n",
      "\n",
      "colonne Client\n",
      "\n",
      "colonne Lieu\n",
      "\n",
      "\n",
      "... fin !\n",
      "Temps d'exécution : 6 minutes 8.691 secondes\n"
     ]
    }
   ],
   "source": [
    "# Recherche des mots interdits\n",
    "# -----------------------------    \n",
    "df_interdit = create_df_interdit(table_osirisk=df_osirisk_1,\n",
    "                                 columns=columns,\n",
    "                                 motsInterdits=motsInterdits_1,\n",
    "                                 prenoms=prenoms_1,\n",
    "                                 print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83642e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du résultat au format pickle\n",
    "\"\"\"\n",
    "with open('temp_result/df_resultat_meth1.pkl', 'wb') as f:\n",
    "    pickle.dump(df_interdit, f)\n",
    "\"\"\"    \n",
    "# Récupération de la sauvegarde pickle  \n",
    "with open('temp_result/df_resultat_meth1.pkl', 'rb') as f:\n",
    "    df_interdit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b54cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_excel(file_name,df_interdit,columns):\n",
    "    \"\"\"\n",
    "    Cette fonction écrit les résultats obtenus par la fonction create_df_interdit()\n",
    "    dans un tableau excel. \n",
    "    Chaque feuille de l'excel correspond à une dataframe de la liste de dataframe renvoyée par \n",
    "    create_df_interdit(). Les mots interdits sont écris en rouge\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    - file_name (string) : nom du fichier excel que l'on veut créer.\n",
    "    - df_interdit (list of dataframes) : liste de dataframes renvoyées par la\n",
    "      fonction create_df_interdit()\n",
    "    - columns (list of strings) : liste des colonnes pour lequelles on recherche \n",
    "      les mots interdits. \n",
    "      \n",
    "    Output:\n",
    "    ------\n",
    "    - fichier excel où chaque feuille correspond à une dataframe de df_interdit.\n",
    "      Chaque feuille porte le nom d'une colonne de la base osirisk pour dans lesuquelles\n",
    "      on a recherché des mots interdits.          \n",
    "    \n",
    "    Reference:\n",
    "    ----------\n",
    "    Ecrire la couleur dans un fichier excel:\n",
    "    - https://github.com/jmcnamara/XlsxWriter/issues/346\n",
    "    \"\"\"\n",
    "\n",
    "    # Création du document excel\n",
    "    workbook = xlsxwriter.Workbook(file_name + '.xlsx')\n",
    "\n",
    "    # Définition des couleurs\n",
    "    red = workbook.add_format({'color': 'red'})\n",
    "    #blue = workbook.add_format({'color': 'blue'})\n",
    "    text_wrap = workbook.add_format({'text_wrap': True})\n",
    "\n",
    "    # ###################################\n",
    "    # Parcours de chaque dataframe df_i de df_interdit\n",
    "    # ###################################\n",
    "    for i in range(len(df_interdit)):\n",
    "        worksheet = workbook.add_worksheet(columns[i])\n",
    "        df_i = df_interdit[i]\n",
    "\n",
    "        # ***********************************\n",
    "        # Parcours de chaque ligne de df_i\n",
    "        # ***********************************\n",
    "        for j in range(len(df_i)):\n",
    "            texte = df_i[\"texte\"].iloc[j]\n",
    "            num_incident = df_i[\"numero incident\"].iloc[j]\n",
    "            mot_int = df_i[\"mots interdits\"].iloc[j] #peut être 1 string ou une liste de strings\n",
    "            idx_osirisk = df_i[\"index osirik\"].iloc[j]\n",
    "\n",
    "            # ===================================\n",
    "            # Ecriture de l'entête du document excel\n",
    "            # ===================================\n",
    "            bold = workbook.add_format({'bold': True}) #bold format\n",
    "            for jj, t in enumerate(df_i.columns):\n",
    "                worksheet.write(0, jj, t, bold)\n",
    "            # ===================================\n",
    "\n",
    "            # ===================================\n",
    "            # Création du texte en couleur\n",
    "            # ===================================\n",
    "            # on a un texte avec des mots interdits : \n",
    "            # texte = \"blabla mot_interdit_1 bloblo mot_interdit_2 blublu mot_iterdit_3 blibli\"\n",
    "            # on veut ajouter la couleur red avant chaque mot interdit de telle sorte : \n",
    "            # string_parts = [\"blabla\", red, \"mot_interdit_1\", \"bloblo\" , red, \n",
    "            # \"mot_interdit_2\" ,\"blublu\" ,red,\"mot_iterdit_3\",\"blibli\"]\n",
    "\n",
    "            # si mot_int n'est pas une liste, on en fait une liste\n",
    "            if isinstance(mot_int, list)==False:\n",
    "                mot_int_ = [mot_int]\n",
    "                mot_to_write = mot_int\n",
    "            else:\n",
    "                mot_int_ = mot_int\n",
    "                mot_to_write = (', ').join(mot_int)\n",
    "            \n",
    "            for mi in mot_int_:\n",
    "                # remplace le mot interdit par \"red\" + le mot interdit\n",
    "                texte = re.sub(r\"\\b\"+ mi + r\"\\b\", \"red \" + mi, texte)\n",
    "            string_parts = texte.split() \n",
    "            # remplace \"red\" par red\n",
    "            string_parts_ = [\" \" +x+\" \" if x !='red' else red for x in string_parts]\n",
    "            string_parts_.append(text_wrap)\n",
    "            \n",
    "            # ===================================\n",
    "\n",
    "            # ===================================\n",
    "            # Ecriture des informations sur la feuille excel\n",
    "            # ===================================\n",
    "            row = j+1 #ligne j + entête\n",
    "            worksheet.write(row,0,num_incident)                                                         \n",
    "            worksheet.write(row,1,mot_to_write) #row, col, *args\n",
    "            worksheet.write_rich_string(row,2, *string_parts_) #row, col, *string_parts\n",
    "            worksheet.write(row,3,idx_osirisk) #row, col, *args\n",
    "            # ===================================\n",
    "        info_txt = str(len(df_i)) + ' mots interdits trouvés'\n",
    "        worksheet.write(0,4,info_txt,bold) #row, col, *args \n",
    "        # ***********************************\n",
    "    # ###################################\n",
    "\n",
    "    # Fermeture du document excel\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c94b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A3193307\\AppData\\Local\\miniforge3\\envs\\motsInterdits\\lib\\site-packages\\xlsxwriter\\worksheet.py:1374: UserWarning: Excel doesn't allow 2 consecutive formats in rich strings. Ignoring input in write_rich_string().\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Sauvagarde du résultat sous excel + coloration des mots interdits\n",
    "# -----------------------------    \n",
    "write_results_excel(\"Python_resultat_meth1_sans_lemmatisation\",df_interdit,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2754205",
   "metadata": {},
   "source": [
    "## Méthode 2 : avec lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c049396",
   "metadata": {},
   "source": [
    "**Méthode utilisée dans cette partie :**\n",
    "  \n",
    "Dans cette partie, nous lemmatisons le texte de la table Osirisk ainsi que le texte des motsInterdits. Puis nous effectuons un pre-traitement de la table Osirisk et des motsInterdits (retrait accents, majuscules, ponctuation...). Le pre-traitement est effectué après la lemmatisation car celui-ci pourrait fausser la compréhension du texte nécessaire pour la lemmatisation, en retirant par exemple les accents (branchées deviendrait branchees). \\\n",
    "Une fois ces étapes réalisées, nous recherchons les motsInterdits (lemmatisés + pré-traités) qui se trouvent dans la table Osirisk (lemmatisée + pré-traitée). \\\n",
    "Cependant, nous remarquons que la lemmatisation modifie certains prénoms (véronique devient véroniqu), ce qui empêche ensuite de ne pas considérer comme mot interdit les mots qui sont entourés par des prénoms. Pour pallier ce problème, nous avons tenté une première approche qui consistait à lemmatiser les mots de la base Osirik seulement si ceux-ci n'étaient pas des prénoms. La lemmatisation prend déjà beaucoup de temps à calculer et cette première approche prennait encore plus de temps car en plus de la lemmatisation, il fallait rechercher si chaque mot de la base Osirik était ou pas dans la liste des prénoms. \\\n",
    "De fait, nous avons implémenté une deuxième approche qui consiste à lemmatiser en plus les prénoms. Ainsi, lorsque nous recherchons si un motInterdit est en réalité un prénom, cela ne pose plus de problème. Cette méthode est bien plus rapide mais un peu moins robuste que la première (par exemple, 'brigitte' seul n'est pas lemmatisé lorsque l'on lemmatise la liste des prénoms, mais est lemmartisé en 'brigitt' dans une phrase). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04a183",
   "metadata": {},
   "source": [
    "**----------------- INSTALLATION DES LIBRAIRIES NECESSAIRES A LA MAIN -----------------**\n",
    "\n",
    "**Tutoriel simple pour le NLP en français** \n",
    "\n",
    "https://maelfabien.github.io/machinelearning/NLPfr/#2-enlever-les-mots-les-plus-fr%C3%A9quents\n",
    "\n",
    "**Installation de Spacy et des modèles Français**\n",
    "\n",
    "\n",
    "Il nous faut un Lemmatizer en Français. Ceci est proposé par la librairie SpaCy. Pour cela, on commence par installer spacy, comme recommandé par ce tutoriel https://maelfabien.github.io/machinelearning/NLPfr/#ii-les-grands-principes : \\\n",
    "`pip install spacy` \\\n",
    "Ensuite, on veut télécharger le modèle français :\\\n",
    "`python -m spacy download fr_core_news_sm` \\\n",
    "Le problème est que l'on est bloqué par la caisse d'épargne. Il faut donc réaliser une installation à la main des modèles français (sources : https://github.com/explosion/spacy-models, https://stackoverflow.com/questions/69216523/spacy-download-en-core-web-lg-manually).\n",
    "\n",
    "Pour télécharger le modèle français, on se rend sur ce lien : https://github.com/explosion/spacy-models/releases/tag/fr_core_news_sm-3.5.0. Descendre en bas de la page sur la rubrique `Assets` et télécharger l'archive `fr_core_news_sm-3.5.0.tar.gz     15.5 MB    Jan 18`. Par exemple, on peut mettre cette archive dans un répertoire `utilities` préalablement créé. \\\n",
    "Une fois l'archive téléchargée, on extrait cette archive. Cela va créer un répertoire `dist` qui contient lui-même un répertoire `._` et un répertoire `fr_core_news_sm-3.5.0`. Lors de l'extraction, il se peut que l'on ait un problème de noms de fichiers pour le répertoire `._`. Cela n'a pas d'importance car le répertoire `._` ne sera pas utilisé. Cliquer sur remplacer les noms ou renommer, peu importe. \\\n",
    "Nous avons la configuration suivante après extraction de `fr_core_news_sm-3.5.0.tar.gz`:\n",
    "\n",
    "    |--- dist\n",
    "    |    |--- ._\n",
    "    |    |--- fr_core_news_sm-3.5.0\n",
    "    |    |    |    |--- fr_core_news_sm\n",
    "    |    |    |    |    |--- fr_core_news_sm-3.5.0\n",
    "    |    |    |    |    |    |--- attribute_ruler\n",
    "    |    |    |    |    |    |--- lemmatizer\n",
    "    |    |    |    |    |    |--- morphologizer\n",
    "    |    |    |    |    |    |--- ner\n",
    "    |    |    |    |    |    |--- parser\n",
    "    |    |    |    |    |    |--- senter\n",
    "    |    |    |    |    |    |--- tok2vec\n",
    "    |    |    |    |    |    |--- vocab\n",
    "    |    |    |    |    |    |--- accuracy.json\n",
    "    |    |    |    |    |    |--- config.cfg\n",
    "    |    |    |    |    |    |--- LICENCE\n",
    "    |    |    |    |    |    |--- LICENSES_SOURCES\n",
    "    |    |    |    |    |    |--- meta.json\n",
    "    |    |    |    |    |    |--- README.md\n",
    "    |    |    |    |    |    |--- tokenizer\n",
    "    |    |    |    |    |---__init__.py\n",
    "    |    |    |    |    |---meta.json\n",
    "    |    |    |--- fr_core_news_sm.egg-info\n",
    "    |    |    |--- meta.json\n",
    "    |    |    |--- LICENSE\n",
    "    |    |    |--- LICENSES_SOURCES\n",
    "    |    |    |--- MANIFEST.in\n",
    "    |    |    |--- PKG-INFO\n",
    "    |    |    |--- README.md\n",
    "    |    |    |--- setup.cfg\n",
    "    |    |    |--- setup.py\n",
    "\n",
    "\n",
    "Nous aurons seulement besoin du répertoire `fr_core_news_sm-3.5.0` situé ici : `dist/fr_core_news_sm-3.5.0/fr_core_news_sm/fr_core_news_sm-3.5.0`, contenant le `lemmatizer`,`ner`,`tokenizer`,... \\\n",
    "On copie tout le répertoire `fr_core_news_sm-3.5.0` situé ici `dist/fr_core_news_sm-3.5.0` et on le place dans `utilities` ou dans le répertoire de votre choix.\n",
    "\n",
    "Dans un notebook Python (sous l'environnement qui convient), écrire les lignes suivantes pour charger le modèle français :\n",
    "\n",
    "`import spacy` \\\n",
    "`nlp = spacy.load(r'chemin_vers_utilities/fr_core_news_sm-3.5.0/fr_core_news_sm/fr_core_news_sm-3.5.0')`\n",
    "\n",
    "\n",
    "_______________\n",
    "Voici les différents modèles français de spacy qui existent : https://spacy.io/models/fr\n",
    "\n",
    "Les suffixes `sm/md/lg` font référence aux tailles des modèles (respectivement petit, moyen et grand).\n",
    "\n",
    "Les différences entre les modèles sont essentiellement statistiques. En général, nous nous attendons à ce que les modèles plus grands soient \"meilleurs\" et plus précis. En fin de compte, cela dépend de notre cas d'utilisation et de nos besoins. Nous recommandons de commencer par les modèles par défaut.\n",
    "\n",
    "Pour information, le modèle `sm` est le modèle par défaut. \\\n",
    "[source : https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod]\n",
    "_______________\n",
    "*Remarque*: \n",
    "Si on veut savoir où est installé une librairie sur mon environemment `motsInterdits` : \\\n",
    "Ouvrir un notebook ou un script python sous l'environnement `motsInterdits` et taper les lignes de commande suivantes (pour connaitre l'emplacement par exemple de la librairie `spacy`) : \n",
    "\n",
    "`import spacy` \\\n",
    "`spacy.__file__` \n",
    "\n",
    "_______________\n",
    "\n",
    "**Installation de nltk**\n",
    "\n",
    "Intaller la librairie python nltk avec la commande suivante :\\\n",
    "`pip install nltk`\\\n",
    "Puis dans un notebook python : \\\n",
    "`import ntlk` \\\n",
    "Il faut ensuite charger `nltk data`, mais on est bloqués par la caisse d'épargne lorsque l'on essaie de procéder par la voie\n",
    "classique en tapant par exemple la commande : \\\n",
    "`nltk.download('stopwords')` \n",
    "\n",
    "Il faut donc faire une installation à la main. Pour cela, on utilise le tutoriel https://www.nltk.org/data.html et on regarde les instructions de la rubrique `Manual installation`. \\\n",
    "Selon les instructions du tutoriel, on se rend dans un des répertoires listés par la variable NLTK_DATA. Pour avoir la valeur de NLTK_DATA, on exécute la commande `nltk.data.path` dans un notebook python après avoir importé la librairie nltk. Si le répertoire n'existe pas déjà, on le crée. Par exemple, ici on crée un répertoire `nltk_data` à cet endroit : `'C:\\\\Users\\\\A3193307\\\\AppData\\\\Local\\\\miniforge3\\\\envs\\\\motsInterdits\\\\nltk_data'`. \\\n",
    "Dans le répertoire `nltk_data`, on crée les sous-répertoires `chunkers`, `grammars`, `misc`, `sentiment`, `taggers`, `corpora`, `help`, `models`, `stemmers`, `tokenizers`. \\\n",
    "Il faudra télécharger individuellement chaque package nécessaire depuis le lien https://www.nltk.org/nltk_data/. Puis dézipper le package et le placer dans le sous-répertoire de `nltk_data` approprié. Par exemple, si on veut télécharger les `stopwords`, on se rend sur le lien https://www.nltk.org/nltk_data/ et on trouve le fichier correspondant, ici intitulé `Stopwords Corpus [ download | source ]`. On effectue un clic droit sur le mot `download` et on clique sur `Copier le lien` pour obtenir le lien de téléchargement de `stopwords`, ici `https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip`. Ce lien de téléchargement nous indique dans quel sous-répertoire placer `stopwords`, ici dans le sous-répertoire `corpora`. \n",
    "On peut maintenant télécharger le fichier `stopwords` et le dézipper dans le répertoire `corpora` de `nltk_data`. Une fois le fichier dézippé, on peut supprimer le .zip et ne garder que le fichier dézippé. \n",
    "\n",
    "On peut à présent utiliser les `stopwords` par exemple : \\\n",
    "`from nltk.corpus import stopwords` \\\n",
    "`stopWords = set(stopwords.words('french'))`\n",
    "\n",
    "**----------------- FIN DE L'INSTALLATION -----------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256ff06",
   "metadata": {},
   "source": [
    "**Ressources**\n",
    "\n",
    "- Liste des prénoms français de 1900 à 2021 : \\\n",
    "https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#:~:text=20%2F06%2F2022-,Le%20fichier%20des%20pr%C3%A9noms%20contient%20des%20donn%C3%A9es%20sur%20les%20pr%C3%A9noms,niveau%20France%20et%20par%20d%C3%A9partement.&text=Les%20fichiers%20propos%C3%A9s%20en%20t%C3%A9l%C3%A9chargement,personnes%20vivantes%20une%20ann%C3%A9e%20donn%C3%A9e.\n",
    "\n",
    "\n",
    "- Ce que veux dire chaque fichier quand on dowload une librairie open-source : https://stackoverflow.com/questions/22842691/what-is-the-meaning-of-the-dist-directory-in-open-source-projects\n",
    "\n",
    "\n",
    "\n",
    "- Lemmatisation/stemming/stopWords toujours utile ? If you are going to use frequency based vectorization to convert text to numbers, then removal of stopwords and using stemming/lemmatization looks ok. As it helps reduce the dimensions and there is no issue of the context anyways.\n",
    "If you are going to use embeddings based vectorization to convert text to numbers then removal of stopwords and using stemming/lemmatization does not look ok. As the dimensions anyway get fixed by other means and the context is not lost.\n",
    "https://www.google.com/search?q=nltk+french+lemmatizer&oq=nltk+french+l&aqs=edge.0.0i512j69i57.5964j0j4&sourceid=chrome&ie=UTF-8 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4d6bf4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle français de spacy\n",
    "# -----------------------------   \n",
    "nlp = spacy.load(UTILS + '/fr_core_news_sm-3.5.0/fr_core_news_sm/fr_core_news_sm-3.5.0')\n",
    "\n",
    "# Chargement des stopwords français de nltk\n",
    "# -----------------------------  \n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fa74ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_token(sentence):\n",
    "    \"\"\"\n",
    "    Cette fonction permet de tokeniser un texte.\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    - sentence (string) : texte à tokeniser.\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    - token_list (list of string) : liste des tokens obtenus.    \n",
    "        \n",
    "    =========================== Notes ===========================\n",
    "    La tokenisation consiste à transformer une phrase en mots \n",
    "    ou tokens (qui peuvent ne pas être tout à fait des mots\n",
    "    selon le tokenizer utilisé).\n",
    "    \n",
    "    Exemple : \"J'aime le chocolat, les fraises.\" devient  \n",
    "    [\"J'\", 'aime', 'le', 'chocolat', ',', 'les', 'fraises', '.']\n",
    "    =============================================================\n",
    "    \"\"\"\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    token_list = [token.text for token in doc]\n",
    "    return token_list\n",
    "\n",
    "def return_lemma(sentence):\n",
    "    \"\"\"\n",
    "    Cette fonction permet de lemmatizer tous les mots d'un texte. \n",
    "    Pour la lemmatisation, on utilise la librairie spacy et le modèle fr_core_news_sm.\n",
    "    >>> Utilise la fonction preprocess_text().\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    - sentence (string) : texte à lemmatiser.\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    - sentence_lemma (string) : texte lemmatisé.\n",
    "    \n",
    "    =========================== Notes ===========================\n",
    "    Il existe deux façons principales de normaliser un texte. \n",
    "    1)--- Le stemming. \n",
    "    Le stemming est le processus qui consiste à dériver ou à supprimer les \n",
    "    derniers caractères d'un mot, ce qui conduit souvent à des significations\n",
    "    et à des orthographes incorrectes. On retire les stopWords avant le stemming\n",
    "    \n",
    "    Certains mots issus du stemming peuvent \n",
    "    ne vouloir rien dire et ne pas être des mots réels. \n",
    "    Le stemming est utilisé dans le cas de grands ensembles de données où la \n",
    "    performance est un problème. Pour palier au problème du stemming, on utilise\n",
    "    la lemmatisation. \n",
    "    2)--- La lemmatisation \n",
    "    La lemmatisation prend en compte le contexte et convertit le mot en\n",
    "    sa forme de base significative, appelée lemme. Les mots issus de la lemmatisation\n",
    "    sont des mots réels. \n",
    "    La lemmatisation est coûteuse en termes de calcul, car elle comprend des tables \n",
    "    de recherche et nécessite beaucoup de connaissances et de compréhension de la structure\n",
    "    d'une langue.  \n",
    "    \n",
    "    Normalement, avant de procéder à la lemmatisation ou au stemming, on convertit \n",
    "    les phrases en tokens (~ mots). \n",
    "    La tokenization permet par exemple de tranformer : \n",
    "    \"J'aime le chocolat, les fraises et la crème\" en \n",
    "    [\"J'\", 'aime', 'le', 'chocolat', ',', 'les', 'fraises', 'et', 'la', 'crème']\n",
    "    Cependant, ici Il n'y a pas besoin de tokenizer au préalable le texte, la librairie\n",
    "    spacy le fait automatiquement avant d'appliquer la lemmatisation. \n",
    "    \n",
    "    La lemmatisation retire les majuscules, et les articles \"l'\".\n",
    "    =============================================================\n",
    "    \n",
    "    Référence:\n",
    "    ----------\n",
    "    French lemmatizer :\n",
    "    - https://maelfabien.github.io/machinelearning/NLPfr/#\n",
    "    - https://stackoverflow.com/questions/13131139/lemmatize-french-text\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    sentence_lemma = ' '.join([token.lemma_ for token in doc])\n",
    "    return sentence_lemma\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Cette fonction permet de retirer les stopwords d'un texte. \n",
    "    >>> Utilise la fonction return_token()\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    - sentence (string) : texte pour lequel retirer les stopwords.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    - sentence_clean (string) : texte pour lequel on a retiré les stopwords.\n",
    "    \n",
    "    =========================== Notes ===========================\n",
    "    Certains mots se retrouvent très fréquemment dans la langue française. \n",
    "    En anglais, on les appelle les “stop words”. Ces mots, bien souvent, n’apportent pas\n",
    "    d’information dans certaines tâches de NLP. \n",
    "    Les “stop words” sont établis comme des listes de mots. \n",
    "    Ces listes sont généralement disponibles dans une librairie appelée NLTK\n",
    "    =============================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_words = []\n",
    "    \n",
    "    # on parcourt chaque mot de la phrase sentence\n",
    "    # chaque mot est obtenu en tokenisant. \n",
    "    for token in return_token(sentence):\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n",
    "            \n",
    "    sentence_clean = ' '.join(clean_words)\n",
    "    return sentence_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c9904a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Texte original :\n",
      " Le vif renard brun saute par-dessus le chien paresseux. L'oiseau est posé sur une branche.\n",
      "---- Retrait stopwords:\n",
      " Le vif renard brun saute - dessus chien paresseux . L' oiseau posé branche .\n",
      "---- Lemmatisation :\n",
      " le vif renard brun saute par - dessus le chien paresseux . le oiseau être poser sur un branche .\n",
      "---- Retrait stopwords puis lemmatisation :\n",
      " le vif renard brun saute - dessus chien paresseux . l ' oiseau poser branch .\n",
      "---- Lemmatisation puis retrait stopwords :\n",
      " vif renard brun saute - dessus chien paresseux . oiseau être poser branche .\n"
     ]
    }
   ],
   "source": [
    "# Test des fonctions remove_stopwords() et return_lemma()\n",
    "# -----------------------------  \n",
    "texte = \"Le vif renard brun saute par-dessus le chien paresseux. L'oiseau est posé sur une branche.\"\n",
    "print(\"---- Texte original :\\n\", texte)\n",
    "\n",
    "texte_sw = remove_stopwords(texte)\n",
    "print(\"---- Retrait stopwords:\\n\", texte_sw)\n",
    "\n",
    "texte_lm = return_lemma(texte)\n",
    "print(\"---- Lemmatisation :\\n\", texte_lm)\n",
    "\n",
    "texte_sw_lm = return_lemma(texte_sw)\n",
    "print(\"---- Retrait stopwords puis lemmatisation :\\n\", texte_sw_lm)\n",
    "\n",
    "texte_lm_sw = remove_stopwords(texte_lm)\n",
    "print(\"---- Lemmatisation puis retrait stopwords :\\n\", texte_lm_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eca6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /.\\ Attention, cette cellule est longue à tourner /.\\\n",
    "# Lemmatisation du texte de la table \n",
    "# -----------------------------\n",
    "'''\n",
    "df_osirisk_2 = df_osirisk.copy()\n",
    "for col in columns: \n",
    "    print(\"\\ncolonne\", col)\n",
    "    \n",
    "    # retire les nan avant la lemmatisation\n",
    "    df_osirisk_2[col] = df_osirisk_2[col].apply(lambda x : preprocess_text(x,['n']))\n",
    "    \n",
    "    # lemmatisation (sans drawProgressBar)\n",
    "    df_osirisk_2[col] = df_osirisk_2[col].apply(lambda x : return_lemma(sentence=x))\n",
    "    \n",
    "    # lemmatisation (avec drawProgressBar)\n",
    "    \"\"\"for j in range(len(df_osirisk_2)):\n",
    "        add_text = ' ' + str(j) + '/' + str(len(df_osirisk_2))\n",
    "        drawProgressBar(j/len(df_osirisk_2), add_text=add_text, barLen=20)\n",
    "        df_osirisk_2[col].iloc[j] = return_lemma(sentence=df_osirisk_2[col].iloc[j])\n",
    "    drawProgressBar(j/len(df_osirisk_2), add_text=add_text, barLen=20)\"\"\"\n",
    "       \n",
    "'''\n",
    "# Enregistrement/Récupération du résultat au format pkl pour utilisation ultérieure\n",
    "# -----------------------------    \n",
    "# Enregistrement du résultat\n",
    "# df_osirisk_2.to_pickle(\"temp_result/df_osirisk_apres_lemmatisation.pkl\")\n",
    "# Récupération du résultat\n",
    "df_osirisk_2 = pd.read_pickle(\"temp_result/df_osirisk_apres_lemmatisation.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147e7f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Incident</th>\n",
       "      <th>Libellé Incident</th>\n",
       "      <th>Description</th>\n",
       "      <th>Réclamation</th>\n",
       "      <th>Local</th>\n",
       "      <th>Client</th>\n",
       "      <th>Lieu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163930</td>\n",
       "      <td>anomalie de kg0r1 de le pas supérieur à 3 mois</td>\n",
       "      <td>anomalie de kg0r1 de le pas supérieur à 3 mois...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163932</td>\n",
       "      <td>abdelkader NETADJ Abbou</td>\n",
       "      <td>erreur agencer sur contrat IARD auto .</td>\n",
       "      <td>045594</td>\n",
       "      <td></td>\n",
       "      <td>04384749402</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163971</td>\n",
       "      <td>SIMON Jeanine</td>\n",
       "      <td>sur avis de dju NOUS indemniser ce client QUI ...</td>\n",
       "      <td>045358</td>\n",
       "      <td></td>\n",
       "      <td>04174259503</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163999</td>\n",
       "      <td>do santos Chrystelle</td>\n",
       "      <td>problème de gestion interne suite IB .</td>\n",
       "      <td>046951</td>\n",
       "      <td></td>\n",
       "      <td>04242148082</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164022</td>\n",
       "      <td>FELIX Jacques</td>\n",
       "      <td>erreur cemp sur cni rb espec</td>\n",
       "      <td>13135201708044633</td>\n",
       "      <td></td>\n",
       "      <td>04500386738</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Incident                                Libellé Incident  \\\n",
       "0      163930  anomalie de kg0r1 de le pas supérieur à 3 mois   \n",
       "1      163932                         abdelkader NETADJ Abbou   \n",
       "2      163971                                   SIMON Jeanine   \n",
       "3      163999                            do santos Chrystelle   \n",
       "4      164022                                   FELIX Jacques   \n",
       "\n",
       "                                         Description        Réclamation Local  \\\n",
       "0  anomalie de kg0r1 de le pas supérieur à 3 mois...                            \n",
       "1             erreur agencer sur contrat IARD auto .             045594         \n",
       "2  sur avis de dju NOUS indemniser ce client QUI ...             045358         \n",
       "3             problème de gestion interne suite IB .             046951         \n",
       "4                       erreur cemp sur cni rb espec  13135201708044633         \n",
       "\n",
       "        Client Lieu  \n",
       "0                    \n",
       "1  04384749402  105  \n",
       "2  04174259503  507  \n",
       "3  04242148082  440  \n",
       "4  04500386738   30  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_osirisk_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ffe4ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement du texte de la table \n",
    "# après avoir effectué la lemmatisation pour conserver le sens du texte\n",
    "# (notamment les accents qui peuvent être des conjugaisons et aider la lemmatisation)\n",
    "# ----------------------------- \n",
    "for col in columns: \n",
    "    # pré-traitement\n",
    "    df_osirisk_2[col] = df_osirisk_2[col].apply(lambda x : preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1791be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement des prénoms\n",
    "# -----------------------------   \n",
    "prenoms_2 = prenoms.copy()\n",
    "prenoms_2 = [preprocess_text(pre) for pre in prenoms_2]\n",
    "\n",
    "# Lemmatisation des prénoms\n",
    "# ----------------------------- \n",
    "prenoms_lemma = []\n",
    "for pre in prenoms_2:\n",
    "        prenoms_lemma.append(return_lemma(sentence=pre))\n",
    "        \n",
    "# suppression des duplicatas qui ont pu apparaître avec la lemmatisation\n",
    "prenoms_lemma = list(dict.fromkeys(prenoms_lemma)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "cb15e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation des mots interdits\n",
    "# -----------------------------    \n",
    "motsInterdits_2 = motsInterdits.copy()\n",
    "motsInterdits_2 = [return_lemma(mot) for mot in motsInterdits_2]\n",
    "\n",
    "# Pré-traitement des mots interdits\n",
    "# -----------------------------    \n",
    "motsInterdits_2 = [preprocess_text(mot) for mot in motsInterdits_2]\n",
    "\n",
    "# suppression des duplicatas qui ont pu apparaître avec la lemmatisation ou le pre-traitement\n",
    "motsInterdits_2 = list(dict.fromkeys(motsInterdits_2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7ef3577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debut Analyse mots interdits...\n",
      "\n",
      "\n",
      "... fin !\n",
      "Temps d'exécution : 5 minutes 30.722 secondes\n"
     ]
    }
   ],
   "source": [
    "# Recherche des mots interdits\n",
    "# -----------------------------    \n",
    "df_interdit_2 = create_df_interdit(table_osirisk=df_osirisk_2,\n",
    "                                   columns=columns,\n",
    "                                   motsInterdits=motsInterdits_2,\n",
    "                                   prenoms=prenoms_lemma,\n",
    "                                   print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71c8ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du résultat au formart pickle\n",
    "\"\"\"\n",
    "with open('temp_result/df_resultat_meth2.pkl', 'wb') as f:\n",
    "    pickle.dump(df_interdit_2, f)\n",
    "\"\"\"\n",
    "\n",
    "# Récupération de la sauvegarde pickle \n",
    "with open('temp_result/df_resultat_meth2.pkl', 'rb') as f:\n",
    "    df_interdit_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3921be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero incident</th>\n",
       "      <th>mots interdits</th>\n",
       "      <th>texte</th>\n",
       "      <th>index osirik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172756</td>\n",
       "      <td>protester</td>\n",
       "      <td>refuge protester de mazamet</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172834</td>\n",
       "      <td>vigiclient</td>\n",
       "      <td>vigiclient indisponibilite image cheq</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173074</td>\n",
       "      <td>vigiclient</td>\n",
       "      <td>probleme vigiclient</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173459</td>\n",
       "      <td>vigiclient</td>\n",
       "      <td>dysfonctionnement acce vigiclient</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183245</td>\n",
       "      <td>canaille</td>\n",
       "      <td>association parents de eleve le petit canaille</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>189908</td>\n",
       "      <td>rat</td>\n",
       "      <td>virt saccef non identifie rat</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>339381</td>\n",
       "      <td>boite a lettre</td>\n",
       "      <td>vol espece boite a lettre marssac</td>\n",
       "      <td>4184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>340075</td>\n",
       "      <td>gens de voyage</td>\n",
       "      <td>occupation temporaire de gens de voyage et deg...</td>\n",
       "      <td>4238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>347219</td>\n",
       "      <td>assedic</td>\n",
       "      <td>regul sur paiement assedic novembre</td>\n",
       "      <td>4333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>347233</td>\n",
       "      <td>assedic</td>\n",
       "      <td>ecart cotisation urssaf assedic</td>\n",
       "      <td>4342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>366997</td>\n",
       "      <td>ib</td>\n",
       "      <td>piquepe celine rbt frais ib</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>381293</td>\n",
       "      <td>ib</td>\n",
       "      <td>ib remboursement penalite liberatoire</td>\n",
       "      <td>5155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>382457</td>\n",
       "      <td>jihad</td>\n",
       "      <td>haouari sif el jihad</td>\n",
       "      <td>5235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>382574</td>\n",
       "      <td>jihad</td>\n",
       "      <td>haouari sif el jihad</td>\n",
       "      <td>5270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>601188</td>\n",
       "      <td>islam</td>\n",
       "      <td>kasdi nor el islam</td>\n",
       "      <td>7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>605147</td>\n",
       "      <td>escroc</td>\n",
       "      <td>alerte a le escroc sur retrait frauduleux</td>\n",
       "      <td>8128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>647414</td>\n",
       "      <td>gens de voyage</td>\n",
       "      <td>occupation illicite de parking de conforama de...</td>\n",
       "      <td>9814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>672696</td>\n",
       "      <td>abus</td>\n",
       "      <td>abus de faiblesse vol sur personne vulnerable ...</td>\n",
       "      <td>14483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>713856</td>\n",
       "      <td>islam</td>\n",
       "      <td>elderbaev islam</td>\n",
       "      <td>36885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>716923</td>\n",
       "      <td>negre</td>\n",
       "      <td>negre brigitt</td>\n",
       "      <td>39951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>719319</td>\n",
       "      <td>islam</td>\n",
       "      <td>islam fatouma</td>\n",
       "      <td>42347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>720105</td>\n",
       "      <td>islam</td>\n",
       "      <td>islam fatouma</td>\n",
       "      <td>43132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>723234</td>\n",
       "      <td>protester</td>\n",
       "      <td>institut protester</td>\n",
       "      <td>45275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>792690</td>\n",
       "      <td>a le etranger</td>\n",
       "      <td>refus de autorisation systematique pour certai...</td>\n",
       "      <td>45874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>794612</td>\n",
       "      <td>fichage bdf</td>\n",
       "      <td>mauvais detection de homonymie sur fichage bdf</td>\n",
       "      <td>45901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>851207</td>\n",
       "      <td>escroc</td>\n",
       "      <td>alerte a le escroc retraits frauduleux</td>\n",
       "      <td>47026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>857274</td>\n",
       "      <td>[le greffe, greffe]</td>\n",
       "      <td>erreur de greffe pour le enregistrement de un ...</td>\n",
       "      <td>47627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1075021</td>\n",
       "      <td>abus</td>\n",
       "      <td>edpl abus de confiance</td>\n",
       "      <td>49153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1094870</td>\n",
       "      <td>greffe</td>\n",
       "      <td>erreur reglement par lettre cheque greffe avig...</td>\n",
       "      <td>49245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1101347</td>\n",
       "      <td>[ficp, fichage ficp]</td>\n",
       "      <td>dysfonctionnement fichage defichage ficp sur l...</td>\n",
       "      <td>49310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1143523</td>\n",
       "      <td>abus</td>\n",
       "      <td>barthe christian fraude cb par abus de confiance</td>\n",
       "      <td>49808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    numero incident        mots interdits  \\\n",
       "0            172756             protester   \n",
       "1            172834            vigiclient   \n",
       "2            173074            vigiclient   \n",
       "3            173459            vigiclient   \n",
       "4            183245              canaille   \n",
       "5            189908                   rat   \n",
       "6            339381        boite a lettre   \n",
       "7            340075        gens de voyage   \n",
       "8            347219               assedic   \n",
       "9            347233               assedic   \n",
       "10           366997                    ib   \n",
       "11           381293                    ib   \n",
       "12           382457                 jihad   \n",
       "13           382574                 jihad   \n",
       "14           601188                 islam   \n",
       "15           605147                escroc   \n",
       "16           647414        gens de voyage   \n",
       "17           672696                  abus   \n",
       "18           713856                 islam   \n",
       "19           716923                 negre   \n",
       "20           719319                 islam   \n",
       "21           720105                 islam   \n",
       "22           723234             protester   \n",
       "23           792690         a le etranger   \n",
       "24           794612           fichage bdf   \n",
       "25           851207                escroc   \n",
       "26           857274   [le greffe, greffe]   \n",
       "27          1075021                  abus   \n",
       "28          1094870                greffe   \n",
       "29          1101347  [ficp, fichage ficp]   \n",
       "30          1143523                  abus   \n",
       "\n",
       "                                                texte  index osirik  \n",
       "0                         refuge protester de mazamet           344  \n",
       "1               vigiclient indisponibilite image cheq           366  \n",
       "2                                 probleme vigiclient           421  \n",
       "3                   dysfonctionnement acce vigiclient           487  \n",
       "4      association parents de eleve le petit canaille           690  \n",
       "5                       virt saccef non identifie rat           771  \n",
       "6                   vol espece boite a lettre marssac          4184  \n",
       "7   occupation temporaire de gens de voyage et deg...          4238  \n",
       "8                 regul sur paiement assedic novembre          4333  \n",
       "9                     ecart cotisation urssaf assedic          4342  \n",
       "10                        piquepe celine rbt frais ib          4689  \n",
       "11              ib remboursement penalite liberatoire          5155  \n",
       "12                               haouari sif el jihad          5235  \n",
       "13                               haouari sif el jihad          5270  \n",
       "14                                 kasdi nor el islam          7945  \n",
       "15          alerte a le escroc sur retrait frauduleux          8128  \n",
       "16  occupation illicite de parking de conforama de...          9814  \n",
       "17  abus de faiblesse vol sur personne vulnerable ...         14483  \n",
       "18                                    elderbaev islam         36885  \n",
       "19                                      negre brigitt         39951  \n",
       "20                                      islam fatouma         42347  \n",
       "21                                      islam fatouma         43132  \n",
       "22                                 institut protester         45275  \n",
       "23  refus de autorisation systematique pour certai...         45874  \n",
       "24     mauvais detection de homonymie sur fichage bdf         45901  \n",
       "25             alerte a le escroc retraits frauduleux         47026  \n",
       "26  erreur de greffe pour le enregistrement de un ...         47627  \n",
       "27                             edpl abus de confiance         49153  \n",
       "28  erreur reglement par lettre cheque greffe avig...         49245  \n",
       "29  dysfonctionnement fichage defichage ficp sur l...         49310  \n",
       "30   barthe christian fraude cb par abus de confiance         49808  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interdit_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a1045e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A3193307\\AppData\\Local\\miniforge3\\envs\\motsInterdits\\lib\\site-packages\\xlsxwriter\\worksheet.py:1374: UserWarning: Excel doesn't allow 2 consecutive formats in rich strings. Ignoring input in write_rich_string().\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Sauvagarde du résultat sous excel + coloration des mots interdits\n",
    "# -----------------------------    \n",
    "write_results_excel(\"Python_resultat_meth2_avec_lemmatisation\",df_interdit_2,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56e288",
   "metadata": {},
   "source": [
    "# Idées poursuite du projet/consignes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56b93b",
   "metadata": {},
   "source": [
    "**Détection d'insultes en Français avec BERT**\n",
    "\n",
    "-- Essayer une correction d'orthographe et des typos pour améliorer les résultats \n",
    "\n",
    "-- Analyse de sentiment sur texte + interprétabilité pour voir quels mots ont participé à la décision et voir si ces mots matchent avec la liste de mots interdits\n",
    "\n",
    "\n",
    "-- Récupérer un réseau de neurones pour le NLP (ex: BERT) entraîné sur des insultes en Français. Puis générer des phrases avec les mots interdits qui sont dans la liste afin de créer notre propre base de données d'entraînement (par exemple en utilisant un modèle génératif comme ChatGPT). Eventuellement faire de la data augmentation sur ces phrases. Enfin, réetraîner (fine-tuning) un réseau de neurones sur ce dataset créé. \\\n",
    "\n",
    "/.\\ le modèle génératif biaisé ? Attention à avoir un modèle génératif qui fonctionne \\\n",
    "Limite : pas de capacités de calcul (pas de GPU) donc très difficile de réentraîner un modèle. Pas accès à ChatGPT avec l'ordinateur de la banque. \n",
    "\n",
    "Detection of Racist Language in French Tweets : \n",
    "https://www.mdpi.com/2078-2489/13/7/318\n",
    "\n",
    "https://aclanthology.org/2020.lrec-1.175.pdf\n",
    "\n",
    "https://www.researchgate.net/publication/345694077_HurtBERT_Incorporating_Lexical_Features_with_BERT_for_the_Detection_of_Abusive_Language\n",
    "\n",
    "https://www.telecom-valley.fr/wp-content/uploads/2020/11/VILLATA-CABRIO-171120.pdf\n",
    "\n",
    "Hate speech detection on Twitter using transfer learning : https://www.sciencedirect.com/science/article/abs/pii/S0885230822000110\n",
    "\n",
    "\n",
    "Bert en Français : CamemBert, FlowBert\n",
    "\n",
    "-- Traduction du texte en anglais pour avoir de meilleures bases de données\n",
    "\n",
    "-- Pas de GPU ? VertexIA sur GCP -> notebooks dans le cloud. On charge les 50000 lignes et les mots interdits dans Vertex et on entraîne dans un notebook. Mais pour le moment, GCP est bloqué. Cependantr, les data-scientists de BPCE ont accès à la puissance de calcul. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motsInterdits",
   "language": "python",
   "name": "motsinterdits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
